# PagerDuty Integration Configuration

apiVersion: v1
kind: Secret
metadata:
  name: pagerduty-integration
  namespace: monitoring
type: Opaque
stringData:
  service-key: "${PAGERDUTY_SERVICE_KEY}"
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: pagerduty-runbooks
  namespace: monitoring
data:
  high-error-rate.md: |
    # Runbook: High Error Rate
    
    ## Symptoms
    - Error rate exceeds 1% of total requests
    - Users reporting failed message delivery
    
    ## Investigation Steps
    1. Check application logs:
       ```bash
       kubectl logs -n default -l app.kubernetes.io/name=dchat --tail=100
       ```
    
    2. Check error types in Prometheus:
       ```promql
       sum(rate(dchat_errors_total[5m])) by (error_type)
       ```
    
    3. Check database connectivity:
       ```bash
       kubectl exec -n default -it <pod-name> -- psql -h $DATABASE_HOST -U $DATABASE_USER -d $DATABASE_NAME -c "SELECT 1;"
       ```
    
    ## Mitigation
    - If database connection issues: Scale database read replicas
    - If network issues: Check relay node connectivity
    - If code bugs: Roll back to previous version
    
    ## Escalation
    - If unresolved after 15 minutes: Page on-call engineer
    - If widespread outage: Notify incident commander
  
  high-latency.md: |
    # Runbook: High Message Latency
    
    ## Symptoms
    - P95 latency exceeds 500ms
    - Users reporting slow message delivery
    
    ## Investigation Steps
    1. Check resource usage:
       ```bash
       kubectl top pods -n default -l app.kubernetes.io/name=dchat
       ```
    
    2. Check database query performance:
       ```promql
       histogram_quantile(0.95, sum(rate(dchat_database_query_duration_seconds_bucket[5m])) by (le, query))
       ```
    
    3. Check relay queue depth:
       ```promql
       dchat_relay_message_queue_depth
       ```
    
    ## Mitigation
    - If CPU bottleneck: Scale up HPA or increase CPU limits
    - If database slow: Add indexes or scale database
    - If relay queue backing up: Scale relay nodes
    
    ## Escalation
    - If unresolved after 30 minutes: Page database team
  
  pod-not-ready.md: |
    # Runbook: Pod Not Ready
    
    ## Symptoms
    - Pod status shows not ready for >5 minutes
    - Health checks failing
    
    ## Investigation Steps
    1. Check pod events:
       ```bash
       kubectl describe pod -n default <pod-name>
       ```
    
    2. Check pod logs:
       ```bash
       kubectl logs -n default <pod-name> --previous
       ```
    
    3. Check readiness probe:
       ```bash
       kubectl exec -n default <pod-name> -- curl http://localhost:9090/ready
       ```
    
    ## Mitigation
    - If image pull error: Check image repository credentials
    - If crash loop: Roll back to previous version
    - If resource limits: Increase memory/CPU limits
    
    ## Escalation
    - If multiple pods affected: Declare incident
  
  database-pool-exhausted.md: |
    # Runbook: Database Connection Pool Exhausted
    
    ## Symptoms
    - Connection pool >80% utilized
    - Database connection errors in logs
    
    ## Investigation Steps
    1. Check current connection count:
       ```promql
       dchat_database_connections_active
       ```
    
    2. Check for connection leaks:
       ```bash
       kubectl logs -n default -l app.kubernetes.io/name=dchat | grep "connection.*not.*closed"
       ```
    
    3. Check database max_connections:
       ```sql
       SHOW max_connections;
       SELECT count(*) FROM pg_stat_activity;
       ```
    
    ## Mitigation
    - Increase pool size in application config
    - Scale database to support more connections
    - Find and fix connection leaks
    
    ## Escalation
    - If database overloaded: Page database team immediately
